{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.9","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import StratifiedKFold,train_test_split\nfrom catboost import CatBoostClassifier\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\nfrom sklearn.ensemble import RandomForestClassifier,AdaBoostClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import roc_auc_score\n\nimport optuna\nimport pickle\n\nimport warnings\n\nwarnings.filterwarnings('ignore')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* CatBoost (cb) can handle categorical data without any transforms. So I just passed the columns directly to CatBoost unchanged.\n* lightGBM (lgb) can handle categorical data, but only if each label has been transformed to an integer value first. So I passed the label encoded values to LightGBM.\n* For XGBoost (xgb), Ridge (ridge), and Stochastic Gradient Descent (sgd), each of them require continuous data to work.","metadata":{}},{"cell_type":"code","source":"train=pd.read_csv('../input/training-tabular-apr-2021/train2.csv')\ntest=pd.read_csv('../input/training-tabular-apr-2021/test2.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.drop(columns=['PassengerId','Name','Cabin','Ticket','SibSp','Parch','Embarked','Sex'], inplace=True)\ntest.drop(columns=['PassengerId','Name','Cabin','Ticket','SibSp','Parch','Embarked','Sex'], inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train[train.columns[1:]]=train[train.columns[1:]].astype(np.float32)\ntest=test.astype(np.float32)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"np.isfinite(train).any()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"np.isfinite(test).any()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"np.isnan(train).any()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"np.isnan(test).any()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h2 style=\"background-color:azure; text-align:center; font-size:300%\">5. Model Tuning</h2>","metadata":{}},{"cell_type":"markdown","source":"<h2 style=\"background-color:azure; text-align:center; font-size:200%\">5.1.1. Random Forest Tuning</h2>","metadata":{}},{"cell_type":"code","source":"features=train.columns[1:]\nX = train[features]\ny = train['Survived']\n\nx_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=39)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"folds=3\nSEED=793\nkf=StratifiedKFold(n_splits=folds, random_state=SEED, shuffle=True)\nscore=pd.DataFrame()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Trial=0\ndef objective(trial, x=x_train, y=y_train):\n    global Trial\n    \n    para={\n        'n_estimators': trial.suggest_int('n_estimators',10, 1000),\n        'criterion': 'entropy',\n        'max_depth': trial.suggest_int('max_depth', 3, 200),\n        'min_samples_split': trial.suggest_float('min_samples_split', 1e-4, 1e-1),\n        'min_samples_leaf': trial.suggest_float('min_samples_leaf', 1e-4, 1e-1),\n        'max_features': trial.suggest_categorical(\"max_features\", ['sqrt', 'log2']),\n        'max_leaf_nodes': trial.suggest_int('max_leaf_nodes', 8, 10000),\n        'min_impurity_decrease': trial.suggest_float('min_impurity_decrease', 0, 0.1),\n        'bootstrap': True,\n        'n_jobs': -1,\n        'verbose': True,\n        'class_weight':trial.suggest_categorical(\"class_weight\", ['balanced', 'balanced_subsample']),\n        'max_samples': trial.suggest_float('max_samples', 0.5, 1),\n        'random_state': SEED\n    }\n    \n    print(\"--------------------> Trial {} <--------------------\".format(Trial))\n    Trial=Trial + 1\n    rf_train_preds = np.zeros(len(y),)\n    for fold, (train_ind, val_ind) in enumerate(kf.split(x, y)):\n        print(\"--> Fold {}\".format(fold + 1))\n        xtrain, xval = x.iloc[train_ind], x.iloc[val_ind]\n        ytrain, yval = y.iloc[train_ind], y.iloc[val_ind]\n        rf = RandomForestClassifier(**para)\n\n        model =  rf.fit(xtrain, ytrain)\n        pred_train = model.predict_proba(xtrain)[:,1]\n        pred_val = model.predict_proba(xval)[:,1]\n        rf_train_preds[val_ind]=pred_val\n        score1 = roc_auc_score(ytrain, pred_train)\n        score2 = roc_auc_score(yval, pred_val)\n        print('Fold {} AUC Train: {} Validation: {}'.format(fold+1, score1, score2))\n    \n    auc=roc_auc_score(y, rf_train_preds)\n    print('OOF AUC: {}'.format(auc))\n    return auc","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"study=optuna.create_study(study_name=\"Random Forest Optimization\", direction='maximize')\nstudy.optimize(objective, n_trials=20)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trial = study.best_trial\nprint('Accuracy: {}'.format(trial.value))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Best hyperparameters: {}\".format(trial.params))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"optuna.visualization.plot_optimization_history(study)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"optuna.visualization.plot_slice(study)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(study.trials_dataframe())\nstudy.trials_dataframe().to_csv(\"trial_parameters.csv\", index=False)\nwith open('./study.pickle', 'wb') as f:\n    pickle.dump(study, f)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"folds=3\nSEED=5661\nkf=StratifiedKFold(n_splits=folds, random_state=SEED, shuffle=True)\nscore=pd.DataFrame()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h2 style=\"background-color:azure; text-align:center; font-size:200%\">5.1.2. Random Forest tuned model</h2>","metadata":{}},{"cell_type":"code","source":"para={\n      'n_estimators': 898,\n      'criterion': 'entropy',\n      'max_depth': 8, \n      'max_leaf_nodes': 9058,\n      'min_samples_split': 0.0671810090247945, \n      'min_samples_leaf': 0.04742472303688006, \n      'max_features': 'sqrt', \n      'min_impurity_decrease': 0.00010583321874846287,\n      'bootstrap': True,\n      'n_jobs': -1,\n      'verbose': True, \n      'class_weight': 'balanced_subsample', \n      'max_samples': 0.8634669615516827,\n      'random_state': SEED\n}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rf_train_preds = np.zeros(len(y_train),)\nrf_test_preds = np.zeros(len(y_test), )\nrf_TEST_preds = np.zeros(len(test), )\nfor fold, (train_ind, val_ind) in enumerate(kf.split(x_train, y_train)):\n    print(\"--> Fold {}\".format(fold + 1))\n    xtrain, xval = x_train.iloc[train_ind], x_train.iloc[val_ind]\n    ytrain, yval = y_train.iloc[train_ind], y_train.iloc[val_ind]\n    rf=RandomForestClassifier(**para)\n\n    model =  rf.fit(xtrain, ytrain)\n    pred_train = model.predict_proba(xtrain)[:,1]\n    pred_val = model.predict_proba(xval)[:,1]\n    pred_test = model.predict_proba(x_test)[:,1]\n    pred_TEST = model.predict_proba(test)[:,1]\n    rf_train_preds[val_ind]=pred_val\n    rf_test_preds+= pred_test/folds\n    rf_TEST_preds+= pred_TEST/folds\n    score1 = roc_auc_score(ytrain, pred_train)\n    score2 = roc_auc_score(yval, pred_val)\n    score3 = roc_auc_score(y_test, pred_test)\n    print('Fold {} AUC Train: {:.4f} Validation: {:.4f}'.format(fold+1, score1, score2))\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('OOF AUC Train: {:.2f} Test: {:.2f}'.format(roc_auc_score(y_train, rf_train_preds), roc_auc_score(y_test, rf_test_preds)))\n\nscore['rf'] = rf_TEST_preds","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_=pd.read_csv('../input/tabular-playground-series-apr-2021/test.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df=pd.DataFrame()\ndf['PassengerId']=test_['PassengerId'].values\ndf['Survived']=score['rf']\ndf['Survived']=df['Survived'].apply(lambda x:1 if x>=0.5 else 0)\ndf.to_csv('./rf_tuned.csv',index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h2 style=\"background-color:azure; text-align:center; font-size:200%\">5.1.1. Random Forest psuedo labelling</h2>","metadata":{}},{"cell_type":"code","source":"test_rf=pd.read_csv('../input/score/rf_tuned.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"folds=5\nSEED=793\nkf=StratifiedKFold(n_splits=folds, random_state=SEED, shuffle=True)\nscore=pd.DataFrame()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"features=train.columns[1:]\nX = train[features]\ny = train['Survived']\n\nx_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.10, random_state=39)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X = X.append(test)\ny=y.append(test_rf['Survived'])\n\nx_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.10, random_state=39)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(x_train), len(x_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"para={\n      'n_estimators': 898,\n      'criterion': 'entropy',\n      'max_depth': 8, \n      'max_leaf_nodes': 9058,\n      'min_samples_split': 0.0671810090247945, \n      'min_samples_leaf': 0.04742472303688006, \n      'max_features': 'sqrt', \n      'min_impurity_decrease': 0.00010583321874846287,\n      'bootstrap': True,\n      'n_jobs': 4,\n      'verbose': True, \n      'class_weight': 'balanced_subsample', \n      'max_samples': 0.8634669615516827,\n      'random_state': SEED\n}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rf_pseudo_train_preds = np.zeros(len(y_train),)\nrf_pseudo_test_preds = np.zeros(len(y_test), )\nrf_pseudo_TEST_preds = np.zeros(len(test), )\nfor fold, (train_ind, val_ind) in enumerate(kf.split(x_train, y_train)):\n    print(\"--> Fold {}\".format(fold + 1))\n    xtrain, xval = x_train.iloc[train_ind], x_train.iloc[val_ind]\n    ytrain, yval = y_train.iloc[train_ind], y_train.iloc[val_ind]\n    rf=RandomForestClassifier(**para)\n\n    model =  rf.fit(xtrain, ytrain)\n    pred_train = model.predict_proba(xtrain)[:,1]\n    pred_val = model.predict_proba(xval)[:,1]\n    pred_test = model.predict_proba(x_test)[:,1]\n    pred_TEST = model.predict_proba(test)[:,1]\n    rf_pseudo_train_preds[val_ind]=pred_val\n    rf_pseudo_test_preds+= pred_test/folds\n    rf_pseudo_TEST_preds+= pred_TEST/folds\n    score1 = roc_auc_score(ytrain, pred_train)\n    score2 = roc_auc_score(yval, pred_val)\n    score3 = roc_auc_score(y_test, pred_test)\n    print('Fold {} AUC Train: {:.5f} Validation: {:.5f}'.format(fold+1, score1, score2))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('OOF AUC Train: {:.2f} Test: {:.2f}'.format(roc_auc_score(y_train, rf_pseudo_train_preds), roc_auc_score(y_test, rf_pseudo_test_preds)))\n\nscore['rf_pseudo'] = rf_pseudo_TEST_preds","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_=pd.read_csv('../input/tabular-playground-series-apr-2021/test.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df=pd.DataFrame()\ndf['PassengerId']=test_['PassengerId'].values\ndf['Survived']=score['rf_pseudo']\ndf['Survived']=df['Survived'].apply(lambda x:1 if x>=0.5 else 0)\ndf.to_csv('./rf_pseudo_tuned1.csv',index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h2 style=\"background-color:azure; text-align:center; font-size:200%\">5.1.1. Random Forest psuedo labelling pruning</h2>","metadata":{}},{"cell_type":"code","source":"folds=3\nSEED=793\nkf=StratifiedKFold(n_splits=folds, random_state=SEED, shuffle=True)\nscore=pd.DataFrame()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"features=train.columns[1:]\nX = train[features]\ny = train['Survived']\n\nx_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=39)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Trial=0\ndef objective(trial, x=x_train, y=y_train):\n    global Trial\n    \n    para={\n          'n_estimators': 898,\n          'criterion': 'entropy',\n          'max_depth': 8, \n          'max_leaf_nodes': 9058,\n          'min_samples_split': 0.0671810090247945, \n          'min_samples_leaf': 0.04742472303688006, \n          'max_features': 'sqrt', \n          'min_impurity_decrease': 0.00010583321874846287,\n          'bootstrap': True,\n          'n_jobs': 4,\n          'verbose': True, \n          'class_weight': 'balanced_subsample', \n          'max_samples': 0.8634669615516827,\n          'random_state': SEED,\n          'ccp_alpha': trial.suggest_float('ccp_alpha', 0, 1e-1)\n    }\n    \n    print(\"--------------------> Trial {} <--------------------\".format(Trial))\n    Trial=Trial + 1\n    rf_train_preds = np.zeros(len(y),)\n    for fold, (train_ind, val_ind) in enumerate(kf.split(x, y)):\n        print(\"--> Fold {}\".format(fold + 1))\n        xtrain, xval = x.iloc[train_ind], x.iloc[val_ind]\n        ytrain, yval = y.iloc[train_ind], y.iloc[val_ind]\n        rf=RandomForestClassifier(**para)\n\n        model =  rf.fit(xtrain, ytrain)\n        pred_train = model.predict_proba(xtrain)[:,1]\n        pred_val = model.predict_proba(xval)[:,1]\n        rf_train_preds[val_ind]=pred_val\n        score1 = roc_auc_score(ytrain, pred_train)\n        score2 = roc_auc_score(yval, pred_val)\n        print('Fold {} AUC Train: {} Validation: {}'.format(fold+1, score1, score2))\n    \n    auc=roc_auc_score(y, rf_train_preds)\n    print('OOF AUC: {}'.format(auc))\n    return auc","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"study=optuna.create_study(study_name=\"Random Forest pseudo labelling Optimization\", direction='maximize')\nstudy.optimize(objective, n_trials=20)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trial = study.best_trial\nprint('Accuracy: {}'.format(trial.value))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Best hyperparameters: {}\".format(trial.params))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X = X.append(test)\ny=y.append(test_rf['Survived'])\n\nx_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.10, random_state=39)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(x_train), len(x_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"folds=3\nSEED=793\nkf=StratifiedKFold(n_splits=folds, random_state=SEED, shuffle=True)\nscore=pd.DataFrame()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"para={\n      'n_estimators': 898,\n      'criterion': 'entropy',\n      'max_depth': 8, \n      'max_leaf_nodes': 9058,\n      'min_samples_split': 0.0671810090247945, \n      'min_samples_leaf': 0.04742472303688006, \n      'max_features': 'sqrt', \n      'min_impurity_decrease': 0.00010583321874846287,\n      'bootstrap': True,\n      'n_jobs': 4,\n      'verbose': True, \n      'class_weight': 'balanced_subsample', \n      'max_samples': 0.8634669615516827,\n      'ccp_alpha': 0.0004776691764536976,\n      'random_state': SEED\n}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rf_pseudo1_train_preds = np.zeros(len(y_train),)\nrf_pseudo1_test_preds = np.zeros(len(y_test), )\nrf_pseudo1_TEST_preds = np.zeros(len(test), )\nfor fold, (train_ind, val_ind) in enumerate(kf.split(x_train, y_train)):\n    print(\"--> Fold {}\".format(fold + 1))\n    xtrain, xval = x_train.iloc[train_ind], x_train.iloc[val_ind]\n    ytrain, yval = y_train.iloc[train_ind], y_train.iloc[val_ind]\n    rf=RandomForestClassifier(**para)\n\n    model =  rf.fit(xtrain, ytrain)\n    pred_train = model.predict_proba(xtrain)[:,1]\n    pred_val = model.predict_proba(xval)[:,1]\n    pred_test = model.predict_proba(x_test)[:,1]\n    pred_TEST = model.predict_proba(test)[:,1]\n    rf_pseudo1_train_preds[val_ind]=pred_val\n    rf_pseudo1_test_preds+= pred_test/folds\n    rf_pseudo1_TEST_preds+= pred_TEST/folds\n    score1 = roc_auc_score(ytrain, pred_train)\n    score2 = roc_auc_score(yval, pred_val)\n    score3 = roc_auc_score(y_test, pred_test)\n    print('Fold {} AUC Train: {:.5f} Validation: {:.5f}'.format(fold+1, score1, score2))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('OOF AUC Train: {:.2f} Test: {:.2f}'.format(roc_auc_score(y_train, rf_pseudo1_train_preds), roc_auc_score(y_test, rf_pseudo1_test_preds)))\n\nscore['rf_pseudo_tuned_pruned'] = rf_pseudo1_TEST_preds","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df=pd.DataFrame()\ndf['PassengerId']=test_['PassengerId'].values\ndf['Survived']=score['rf_pseudo_tuned_pruned']\ndf['Survived']=df['Survived'].apply(lambda x:1 if x>=0.5 else 0)\ndf.to_csv('./rf_pseudo_tuned_pruned.csv',index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h2 style=\"background-color:azure; text-align:center; font-size:200%\">5.2.1. AdaBoost tuning</h2>","metadata":{}},{"cell_type":"code","source":"Trial=0\ndef objective(trial, x=x_train, y=y_train):\n    global Trial\n    \n    para={\n        'n_estimators': trial.suggest_int('n_estimators',10, 10000),\n        'learning_rate': trial.suggest_float('learning_rate', 1e-3, 3),\n        'algorithm': 'SAMME.R',\n        'random_state': SEED\n    }\n    \n    print(\"--------------------> Trial {} <--------------------\".format(Trial))\n    Trial=Trial+1\n    ad_train_preds = np.zeros(len(y),)\n    for fold, (train_ind, val_ind) in enumerate(kf.split(x, y)):\n        print(\"--> Fold {}\".format(fold + 1))\n        xtrain, xval = x.iloc[train_ind], x.iloc[val_ind]\n        ytrain, yval = y.iloc[train_ind], y.iloc[val_ind]\n        ad=AdaBoostClassifier(**para)\n\n        model =  ad.fit(xtrain, ytrain)\n        pred_train = model.predict_proba(xtrain)[:,1]\n        pred_val = model.predict_proba(xval)[:,1]\n        ad_train_preds[val_ind]=pred_val\n        score1 = roc_auc_score(ytrain, pred_train)\n        score2 = roc_auc_score(yval, pred_val)\n        print('Fold {} AUC Train: {} Validation: {}'.format(fold+1, score1, score2))\n    \n    auc=roc_auc_score(y, ad_train_preds)\n    print('OOF AUC: {}'.format(auc))\n    return auc","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"study=optuna.create_study(study_name=\"AdaBoost Optimization\", direction='maximize')\nstudy.optimize(objective, n_trials=10)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trial = study.best_trial\nprint('Accuracy: {}'.format(trial.value))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Best hyperparameters: {}\".format(trial.params))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"optuna.visualization.plot_optimization_history(study)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"optuna.visualization.plot_slice(study)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(study.trials_dataframe())\nstudy.trials_dataframe().to_csv(\"trial_parameters.csv\", index=False)\nwith open('./study.pickle', 'wb') as f:\n    pickle.dump(study, f)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h2 style=\"background-color:azure; text-align:center; font-size:200%\">5.2.2. AdaBoost tuned model</h2>","metadata":{}},{"cell_type":"code","source":"para={\n      'n_estimators': 3291,\n      'learning_rate': 0.06936047632110985,\n      'algorithm': 'SAMME.R',\n      'random_state': SEED\n}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ad_train_preds = np.zeros(len(y_train),)\nad_test_preds = np.zeros(len(y_test), )\nad_TEST_preds = np.zeros(len(test), )\nfor fold, (train_ind, val_ind) in enumerate(kf.split(x_train, y_train)):\n    print(\"--> Fold {}\".format(fold + 1))\n    xtrain, xval = x_train.iloc[train_ind], x_train.iloc[val_ind]\n    ytrain, yval = y_train.iloc[train_ind], y_train.iloc[val_ind]\n    ad=AdaBoostClassifier(**para)\n\n    model =  ad.fit(xtrain, ytrain)\n    pred_train = model.predict_proba(xtrain)[:,1]\n    pred_val = model.predict_proba(xval)[:,1]\n    pred_test = model.predict_proba(x_test)[:,1]\n    pred_TEST = model.predict_proba(test)[:,1]\n    ad_train_preds[val_ind]=pred_val\n    ad_test_preds+= pred_test/folds\n    ad_TEST_preds+= pred_TEST/folds\n    score1 = roc_auc_score(ytrain, pred_train)\n    score2 = roc_auc_score(yval, pred_val)\n    score3 = roc_auc_score(y_test, pred_test)\n    print('Fold {} AUC Train: {:.4f} Validation: {:.4f}'.format(fold+1, score1, score2))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('OOF AUC Train: {:.5f} Test: {:.5f}'.format(roc_auc_score(y_train, ad_train_preds), roc_auc_score(y_test, ad_test_preds)))\n\nscore['ad'] = ad_TEST_preds","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_=pd.read_csv('../input/tabular-playground-series-apr-2021/test.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df=pd.DataFrame()\ndf['PassengerId']=test_['PassengerId'].values\ndf['Survived']=score['ad']\ndf['Survived']=df['Survived'].apply(lambda x:1 if x>=0.5 else 0)\ndf.to_csv('./ad_tuned.csv',index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h2 style=\"background-color:azure; text-align:center; font-size:200%\">5.3.1. XGBoost tuning</h2>","metadata":{}},{"cell_type":"code","source":"features=train.columns[1:]\nX = train[features]\ny = train['Survived']\n\nx_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=39)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"folds=3\nSEED=793\nkf=StratifiedKFold(n_splits=folds, random_state=SEED, shuffle=True)\nscore=pd.DataFrame()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Trial=0\ndef objective(trial, x=x_train, y=y_train):\n    global Trial\n    \n    para={\n        'verbosity': 1,\n        'eval_metric': \"auc\",\n        'tree_method':trial.suggest_categorical(\"tree_method\", ['exact', 'approx', 'hist']),\n        'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n        'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.1, 1),\n        'subsample': trial.suggest_float('subsample', 0.1, 1),\n        'learning_rate': trial.suggest_float('learning_rate', 1e-4, 1e-1),\n        'n_estimators': trial.suggest_int('n_estimators',500, 20000),\n        'max_depth': trial.suggest_int('max_depth', 5, 1000),\n        'random_state': 2021,\n        'min_child_weight': trial.suggest_int('min_child_weight', 1, 1000),\n    }\n    \n    print(\"--------------------> Trial {} <--------------------\".format(Trial))\n    Trial=Trial + 1\n    xgboost_train_preds = np.zeros(len(y),)\n    for fold, (train_ind, val_ind) in enumerate(kf.split(x, y)):\n        print(\"--> Fold {}\".format(fold + 1))\n        xtrain, xval = x.iloc[train_ind], x.iloc[val_ind]\n        ytrain, yval = y.iloc[train_ind], y.iloc[val_ind]\n        xgboost = XGBClassifier(**para)\n        \n        model =  xgboost.fit(xtrain, ytrain, eval_set=[(xval, yval)], verbose=1, early_stopping_rounds=50)\n        pred_train = model.predict_proba(xtrain)[:,1]\n        pred_val = model.predict_proba(xval)[:,1]\n        xgboost_train_preds[val_ind]=pred_val\n        score1 = roc_auc_score(ytrain, pred_train)\n        score2 = roc_auc_score(yval, pred_val)\n        print('Fold {} AUC Train: {} Validation: {}'.format(fold+1, score1, score2))\n    \n    auc=roc_auc_score(y, xgboost_train_preds)\n    print('OOF AUC: {}'.format(auc))\n    return auc","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"study=optuna.create_study(study_name=\"XGBoost Optimization\", direction='maximize')\nstudy.optimize(objective, n_trials=15)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trial = study.best_trial\nprint('Accuracy: {}'.format(trial.value))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Best hyperparameters: {}\".format(trial.params))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"optuna.visualization.plot_optimization_history(study)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"optuna.visualization.plot_slice(study)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h2 style=\"background-color:azure; text-align:center; font-size:200%\">5.2.2. XGBoost tuned model</h2>","metadata":{}},{"cell_type":"code","source":"features=train.columns[1:]\nX = train[features]\ny = train['Survived']\n\nx_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=39)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"folds=10\nSEED=793\nkf=StratifiedKFold(n_splits=folds, random_state=SEED, shuffle=True)\nscore=pd.DataFrame()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"para={'eval_metric': \"auc\",'tree_method': 'hist', 'lambda': 0.015235209064507677, 'alpha': 0.015624821169143542, 'colsample_bytree': 0.9102330759766059, 'subsample': 0.5282153709855245, 'learning_rate': 0.023541287073875223, 'n_estimators': 14379, 'max_depth': 794, 'min_child_weight': 161, 'random_state': 2021}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"xgboost_train_preds = np.zeros(len(y_train),)\nxgboost_test_preds = np.zeros(len(y_test), )\nxgboost_TEST_preds = np.zeros(len(test), )\nfor fold, (train_ind, val_ind) in enumerate(kf.split(x_train, y_train)):\n    print(\"--> Fold {}\".format(fold + 1))\n    xtrain, xval = x_train.iloc[train_ind], x_train.iloc[val_ind]\n    ytrain, yval = y_train.iloc[train_ind], y_train.iloc[val_ind]\n    xgboost = XGBClassifier(**para)\n        \n    model =  xgboost.fit(xtrain, ytrain, eval_set=[(xval, yval)], verbose=1, early_stopping_rounds=50)\n    pred_train = model.predict_proba(xtrain)[:,1]\n    pred_val = model.predict_proba(xval)[:,1]\n    pred_test = model.predict_proba(x_test)[:,1]\n    pred_TEST = model.predict_proba(test)[:,1]\n    xgboost_train_preds[val_ind]=pred_val\n    xgboost_test_preds+= pred_test/folds\n    xgboost_TEST_preds+= pred_TEST/folds\n    score1 = roc_auc_score(ytrain, pred_train)\n    score2 = roc_auc_score(yval, pred_val)\n    score3 = roc_auc_score(y_test, pred_test)\n    print('Fold {} AUC Train: {:.4f} Validation: {:.4f}'.format(fold+1, score1, score2))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('OOF AUC Train: {:.5f} Test: {:.5f}'.format(roc_auc_score(y_train, xgboost_train_preds), roc_auc_score(y_test, xgboost_test_preds)))\n\nscore['xgboost'] = xgboost_TEST_preds","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_=pd.read_csv('../input/tabular-playground-series-apr-2021/test.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df=pd.DataFrame()\ndf['PassengerId']=test_['PassengerId'].values\ndf['Survived']=score['xgboost']\ndf['Survived']=df['Survived'].apply(lambda x:1 if x>=0.5 else 0)\ndf.to_csv('./xgboost_tuned.csv',index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}