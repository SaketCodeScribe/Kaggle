{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.9","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import StratifiedKFold, train_test_split\nfrom catboost import CatBoostClassifier\nfrom xgboost import XGBClassifier\nimport lightgbm\nfrom lightgbm import LGBMClassifier\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import roc_auc_score, accuracy_score\nfrom sklearn.preprocessing import OneHotEncoder\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport optuna\n\nimport random\n\nimport warnings\n\nwarnings.filterwarnings('ignore')","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":1,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<style type='text/css'>\n.datatable table.frame { margin-bottom: 0; }\n.datatable table.frame thead { border-bottom: none; }\n.datatable table.frame tr.coltypes td {  color: #FFFFFF;  line-height: 6px;  padding: 0 0.5em;}\n.datatable .bool    { background: #DDDD99; }\n.datatable .object  { background: #565656; }\n.datatable .int     { background: #5D9E5D; }\n.datatable .float   { background: #4040CC; }\n.datatable .str     { background: #CC4040; }\n.datatable .row_index {  background: var(--jp-border-color3);  border-right: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  font-size: 9px;}\n.datatable .frame tr.coltypes .row_index {  background: var(--jp-border-color0);}\n.datatable th:nth-child(2) { padding-left: 12px; }\n.datatable .hellipsis {  color: var(--jp-cell-editor-border-color);}\n.datatable .vellipsis {  background: var(--jp-layout-color0);  color: var(--jp-cell-editor-border-color);}\n.datatable .na {  color: var(--jp-cell-editor-border-color);  font-size: 80%;}\n.datatable .footer { font-size: 9px; }\n.datatable .frame_dimensions {  background: var(--jp-border-color3);  border-top: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  display: inline-block;  opacity: 0.6;  padding: 1px 10px 1px 5px;}\n</style>\n"},"metadata":{}}]},{"cell_type":"markdown","source":"<h2 style=\"background-color:azure; text-align:center; font-size:300%\">Level1 Classification</h2>","metadata":{}},{"cell_type":"markdown","source":"<h2 style=\"background-color:azure; text-align:center; font-size:200%\">XGBOOST</h2>","metadata":{}},{"cell_type":"code","source":"train1=pd.read_csv('../input/training-apr/train.csv')\ntest1=pd.read_csv('../input/training-apr/test.csv')\n\ntrain2=train1.copy()\ntest2=test1.copy()\n\ntrain1.drop(columns=['PassengerId','Name','Cabin','Ticket','SibSp','Parch','Age_log','Fare','Fare_cat','SibSp_log','Parch_log','related','related_cat'], inplace=True)\ntest1.drop(columns=['PassengerId','Name','Cabin','Ticket','SibSp','Parch','Age_log','Fare','Fare_cat','SibSp_log','Parch_log','related','related_cat'], inplace=True)\n\ntrain2.drop(columns=['PassengerId','Name','Cabin','Ticket','SibSp','Parch','Age_log','Fare','Fare_cat','SibSp_log','Parch_log','related','related_log'], inplace=True)\ntest2.drop(columns=['PassengerId','Name','Cabin','Ticket','SibSp','Parch','Age_log','Fare','Fare_cat','SibSp_log','Parch_log','related','related_log'], inplace=True)\n\nprint(\"set 1:\", train1.columns, test1.columns)\nprint(\"set 2:\", train2.columns, test2.columns)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"is finite set1 Train:{} Test:{}\".format(np.isfinite(train1[['Age','Fare_log','related_log']]).any()[0],np.isfinite(test1[['Age','Fare_log','related_log']]).any()[0]))\nprint(\"is finite set2 Train:{} Test:{}\".format(np.isfinite(train2[['Age','Fare_log']]).any()[0],np.isfinite(test2[['Age','Fare_log']]).any()[0]))\n\nprint(\"is nan set1 Train:{} Test:{}\".format(sum(train1.isnull().sum()),sum(test1.isnull().sum())))\nprint(\"is nan set2 Train:{} Test:{}\".format(sum(train2.isnull().sum()),sum(train2.isnull().sum())))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"score_train=pd.DataFrame(columns=['xg','rf','lg','cb']) # train data\nscore_test=pd.DataFrame(columns=['xg','rf','lg','cb']) # leadeboard data","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h2 style=\"background-color:azure; text-align:center; font-size:200%\">XGBOOST set1</h2>","metadata":{}},{"cell_type":"code","source":"ohe=OneHotEncoder()\ncol=['Sex','Embarked']\nohe.fit(train1[col])\nprint(ohe.get_feature_names(col))\ndf1=pd.DataFrame(columns=ohe.get_feature_names(col),data=ohe.transform(train1[col]).toarray())\ndf2=pd.DataFrame(columns=ohe.get_feature_names(col),data=ohe.transform(test1[col]).toarray())\n\ntrain1=train1.join(df1)\ntest1=test1.join(df2)\n\ntrain1.drop(columns=['Sex','Embarked'], inplace=True)\ntest1.drop(columns=['Sex','Embarked'], inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train1.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test1.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"folds=5\nSEED=random.randint(937,8641)\n\nkf=StratifiedKFold(n_splits=folds, random_state=SEED, shuffle=True)\n\nfeatures=train1.columns[1:]\nX = train1[features]\ny = train1['Survived']\n\n\nimbalanced_ratio=(train1[train1['Survived']==0]['Survived'].count()/train1[train1['Survived']==1]['Survived'].count()).round(2)\nprint(\"Imbalnce ratio: {:}\".format(imbalanced_ratio))\nx_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=SEED)\n\nprint(\"Distribution of train and test:\", len(x_train), len(y_train), len(x_test), len(x_test))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Trial=0\ndef objective(trial, x=x_train, y=y_train):\n    global Trial\n    \n    para={\n        'verbosity': 1,\n        'objective': 'binary:logistic',\n        'random_state': SEED,\n        'seed': SEED,\n        'tree_method':'hist',\n        'scale_pos_weight': imbalanced_ratio,\n        'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n        'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.1, 1),\n        'subsample': trial.suggest_float('subsample', 0.1, 1),\n        'learning_rate': trial.suggest_float('learning_rate', 1e-4, 1e-1),\n        'n_estimators': trial.suggest_int('n_estimators',500, 20000),\n        'max_depth': trial.suggest_int('max_depth', 1, 31),\n        'min_child_weight': trial.suggest_int('min_child_weight', 1, 1000)\n    }\n    \n    print(\"--------------------> Trial {} <--------------------\".format(Trial))\n    Trial=Trial + 1\n    xgboost_train_preds = np.zeros(len(y),)\n    for fold, (train_ind, val_ind) in enumerate(kf.split(x, y)):\n        print(\"--> Fold {}\".format(fold + 1))\n        xtrain, xval = x.iloc[train_ind], x.iloc[val_ind]\n        ytrain, yval = y.iloc[train_ind], y.iloc[val_ind]\n        xgboost = XGBClassifier(**para)\n        \n        model =  xgboost.fit(xtrain, ytrain, eval_set=[(xtrain,ytrain), (xval,yval)], \n                             eval_metric=[\"error\", \"logloss\"],verbose=0, early_stopping_rounds=50)\n        pred_train = model.predict(xtrain)\n        pred_val = model.predict(xval)\n        xgboost_train_preds[val_ind]=pred_val\n        score1 = accuracy_score(ytrain, pred_train)\n        score2 = accuracy_score(yval, pred_val)\n        print('Fold {} ACCURACY Train: {} Validation: {}'.format(fold+1, score1, score2))\n        \n        results = model.evals_result()\n        df=pd.DataFrame({\n                        \"validation_train_ll\":results[\"validation_0\"][\"logloss\"],\n                        \"validation_test_ll\":results[\"validation_1\"][\"logloss\"],\n                        \"validation_train_acc\":results[\"validation_0\"][\"error\"],\n                        \"validation_test_acc\":results[\"validation_1\"][\"error\"],\n                        \n        })\n        df['validation_train_acc']=(1-df['validation_train_acc'])*100.0\n        df['validation_test_acc']=(1-df['validation_test_acc'])*100.0\n#         print(df.head())\n        \n        fig, ax = plt.subplots(ncols=2, figsize=(12,6))\n        sns.lineplot(data=df, x=df.index, y=\"validation_train_ll\", ax=ax[0], label=\"Train loss\")\n        sns.lineplot(data=df, x=df.index, y=\"validation_test_ll\", ax=ax[0], label=\"Test loss\")\n        sns.lineplot(data=df, x=df.index, y=\"validation_train_acc\", ax=ax[1], label=\"Train acc.\")\n        sns.lineplot(data=df, x=df.index, y=\"validation_test_acc\", ax=ax[1], label=\"Test acc.\")\n        ax[0].set_title(\"Loss curve\")\n        ax[1].set_title(\"Accuracy curve\")\n        ax[0].set_ylabel(\"Loss\")\n        ax[0].set_xlabel(\"Itertation\")\n        ax[1].set_ylabel(\"Accuracy\")\n        ax[1].set_xlabel(\"Itertation\")\n        fig.suptitle(\"XGBoost Loss/Accuracy.\")\n        plt.show()\n    acc=accuracy_score(y, xgboost_train_preds)\n    print('OOF ACCURACY: {}'.format(acc))\n    return acc","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"study=optuna.create_study(study_name=\"XGBoost set 1 Optimization\", direction='maximize')\nstudy.optimize(objective, n_trials=25)\n\ntrial = study.best_trial\nprint('Accuracy: {}'.format(trial.value))\n\nprint(\"Best hyperparameters: {}\".format(trial.params))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Tuning keeping the N-estimator same as the above best parameter\nTrial=0\ndef objective(trial, x=x_train, y=y_train):\n    global Trial\n    \n    para={\n        'verbosity': 1,\n        'objective': 'binary:logistic',\n        'random_state': SEED,\n        'seed': SEED,\n        'tree_method':'hist',\n        'scale_pos_weight': imbalanced_ratio,\n        'n_estimators': 7657,\n        'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n        'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.1, 1),\n        'subsample': trial.suggest_float('subsample', 0.1, 1),\n        'learning_rate': trial.suggest_float('learning_rate', 1e-4, 1e-1),\n        'max_depth': trial.suggest_int('max_depth', 1, 31),\n        'min_child_weight': trial.suggest_int('min_child_weight', 1, 1000)\n    }\n    \n    print(\"--------------------> Trial {} <--------------------\".format(Trial))\n    Trial=Trial + 1\n    xgboost_train_preds = np.zeros(len(y),)\n    for fold, (train_ind, val_ind) in enumerate(kf.split(x, y)):\n        print(\"--> Fold {}\".format(fold + 1))\n        xtrain, xval = x.iloc[train_ind], x.iloc[val_ind]\n        ytrain, yval = y.iloc[train_ind], y.iloc[val_ind]\n        xgboost = XGBClassifier(**para)\n        \n        model =  xgboost.fit(xtrain, ytrain, eval_set=[(xtrain,ytrain), (xval,yval)], \n                             eval_metric=[\"error\", \"logloss\"],verbose=0, early_stopping_rounds=50)\n        pred_train = model.predict(xtrain)\n        pred_val = model.predict(xval)\n        xgboost_train_preds[val_ind]=pred_val\n        score1 = accuracy_score(ytrain, pred_train)\n        score2 = accuracy_score(yval, pred_val)\n        print('Fold {} ACCURACY Train: {} Validation: {}'.format(fold+1, score1, score2))\n        \n        results = model.evals_result()\n        df=pd.DataFrame({\n                        \"validation_train_ll\":results[\"validation_0\"][\"logloss\"],\n                        \"validation_test_ll\":results[\"validation_1\"][\"logloss\"],\n                        \"validation_train_acc\":results[\"validation_0\"][\"error\"],\n                        \"validation_test_acc\":results[\"validation_1\"][\"error\"],\n                        \n        })\n        df['validation_train_acc']=(1-df['validation_train_acc'])*100.0\n        df['validation_test_acc']=(1-df['validation_test_acc'])*100.0\n#         print(df.head())\n        \n        fig, ax = plt.subplots(ncols=2, figsize=(12,6))\n        sns.lineplot(data=df, x=df.index, y=\"validation_train_ll\", ax=ax[0], label=\"Train loss\")\n        sns.lineplot(data=df, x=df.index, y=\"validation_test_ll\", ax=ax[0], label=\"Test loss\")\n        sns.lineplot(data=df, x=df.index, y=\"validation_train_acc\", ax=ax[1], label=\"Train acc.\")\n        sns.lineplot(data=df, x=df.index, y=\"validation_test_acc\", ax=ax[1], label=\"Test acc.\")\n        ax[0].set_title(\"Loss curve\")\n        ax[1].set_title(\"Accuracy curve\")\n        ax[0].set_ylabel(\"Loss\")\n        ax[0].set_xlabel(\"Itertation\")\n        ax[1].set_ylabel(\"Accuracy\")\n        ax[1].set_xlabel(\"Itertation\")\n        fig.suptitle(\"XGBoost Loss/Accuracy.\")\n        plt.show()\n    acc=accuracy_score(y, xgboost_train_preds)\n    print('OOF ACCURACY: {}'.format(acc))\n    return acc","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"study=optuna.create_study(study_name=\"XGBoost Optimization\", direction='maximize')\nstudy.optimize(objective, n_trials=15)\n\ntrial = study.best_trial\nprint('Accuracy: {}'.format(trial.value))\n\nprint(\"Best hyperparameters: {}\".format(trial.params))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h2 style=\"background-color:azure; text-align:center; font-size:200%\">XGBOOST tuned model set1</h2>","metadata":{}},{"cell_type":"code","source":"para = {\n        'verbosity': 1,\n        'objective': 'binary:logistic',\n        'random_state': SEED,\n        'seed': SEED,\n        'tree_method':'hist',\n        'scale_pos_weight': imbalanced_ratio,\n        'lambda': 0.040336438299178316, \n        'alpha': 1.6115451006296893, \n        'colsample_bytree': 0.6421153349186888, \n        'subsample': 0.9948174596370332, \n        'learning_rate': 0.08617507766633564, \n        'n_estimators': 7657, \n        'max_depth': 18, \n        'min_child_weight': 114\n      }","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"xg_train_preds = np.zeros(len(y_train),)\nxg_test_preds = np.zeros(len(y_test),)\nxg_test = np.zeros(len(test1),)\nfor fold, (train_ind, val_ind) in enumerate(kf.split(x_train, y_train)):\n    print(\"--> Fold {}\".format(fold + 1))\n    \n    xtrain, xval = x_train.iloc[train_ind], x_train.iloc[val_ind]\n    ytrain, yval = y_train.iloc[train_ind], y_train.iloc[val_ind]\n    \n    xgboost = XGBClassifier(**para)\n\n    model = xgboost.fit(xtrain, ytrain, eval_set=[(xtrain,ytrain), (xval,yval)], \n                         eval_metric=[\"error\", \"logloss\"], verbose=0, early_stopping_rounds=50)\n    pred_train = model.predict_proba(xtrain)[:,1]\n    pred_val = model.predict_proba(xval)[:,1]\n    xg_train_preds[val_ind] = pred_val\n    xg_test_preds += (model.predict_proba(x_test)[:,1])/folds\n    xg_test += (model.predict_proba(test1)[:,1])/folds\n    score1 = accuracy_score(ytrain, np.where(pred_train<=0.5, 0, 1))\n    score2 = accuracy_score(yval, np.where(pred_val<=0.5, 0, 1))\n    print('Fold {} ACCURACY Train: {} Validation: {}'.format(fold+1, score1, score2))\n\n    results = model.evals_result()\n    df=pd.DataFrame({\n                    \"validation_train_ll\":results[\"validation_0\"][\"logloss\"],\n                    \"validation_test_ll\":results[\"validation_1\"][\"logloss\"],\n                    \"validation_train_acc\":results[\"validation_0\"][\"error\"],\n                    \"validation_test_acc\":results[\"validation_1\"][\"error\"],\n\n    })\n    df['validation_train_acc']=(1-df['validation_train_acc'])*100.0\n    df['validation_test_acc']=(1-df['validation_test_acc'])*100.0\n    #         print(df.head())\n\n    fig, ax = plt.subplots(ncols=2, figsize=(12,6))\n    sns.lineplot(data=df, x=df.index, y=\"validation_train_ll\", ax=ax[0], label=\"Train loss\")\n    sns.lineplot(data=df, x=df.index, y=\"validation_test_ll\", ax=ax[0], label=\"Test loss\")\n    sns.lineplot(data=df, x=df.index, y=\"validation_train_acc\", ax=ax[1], label=\"Train acc.\")\n    sns.lineplot(data=df, x=df.index, y=\"validation_test_acc\", ax=ax[1], label=\"Test acc.\")\n    ax[0].set_title(\"Loss curve\")\n    ax[1].set_title(\"Accuracy curve\")\n    ax[0].set_ylabel(\"Loss\")\n    ax[0].set_xlabel(\"Itertation\")\n    ax[1].set_ylabel(\"Accuracy\")\n    ax[1].set_xlabel(\"Itertation\")\n    fig.suptitle(\"XGBoost Loss/Accuracy.\")\n    plt.show()\n    \nacc1 = accuracy_score(y_train, np.where(xg_train_preds<=0.5, 0, 1))\nacc2 = accuracy_score(y_test, np.where(xg_test_preds<=0.5, 0, 1))\nprint('OOF ACCURACY Train: {} Test: {}'.format(acc1, acc2))\n\nscore_train['xg']=np.concatenate([xg_train_preds,xg_test_preds])\nscore_test['xg']=xg_test","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"score_train.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"score_test.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"score_train.to_csv('./score_train.csv',index=False)\nscore_test.to_csv('./score_test.csv',index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_=pd.read_csv('../input/tabular-playground-series-apr-2021/test.csv')\ndf=pd.DataFrame()\ndf['PassengerId']=test_['PassengerId'].values\ndf['Survived']=score_test['xg']\ndf['Survived']=df['Survived'].apply(lambda x:0 if x<=0.5 else 1)\ndf.to_csv('./xg_tuned.csv',index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h2 style=\"background-color:azure; text-align:center; font-size:200%\">XGBOOST set2</h2>","metadata":{}},{"cell_type":"code","source":"ohe=OneHotEncoder()\ncol=['Sex','Embarked']\nohe.fit(train2[col])\nprint(ohe.get_feature_names(col))\ndf1=pd.DataFrame(columns=ohe.get_feature_names(col),data=ohe.transform(train2[col]).toarray())\ndf2=pd.DataFrame(columns=ohe.get_feature_names(col),data=ohe.transform(test2[col]).toarray())\n\ntrain2=train2.join(df1)\ntest2=test2.join(df2)\n\ntrain2['related_cat'] = train2['related_cat'].apply(lambda x:1 if x in ['low'] else 2)\ntest2['related_cat'] = test2['related_cat'].apply(lambda x:1 if x in ['low'] else 2)\n\ntrain2.drop(columns=['Sex','Embarked'], inplace=True)\ntest2.drop(columns=['Sex','Embarked'], inplace=True)","metadata":{"_kg_hide-output":false,"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train2.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test2.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"folds=5\nSEED=random.randint(937,8641)\n\nkf=StratifiedKFold(n_splits=folds, random_state=SEED, shuffle=True)\n\nfeatures=train2.columns[1:]\nX = train2[features]\ny = train2['Survived']\n\n\nimbalanced_ratio=(train2[train2['Survived']==0]['Survived'].count()/train2[train2['Survived']==1]['Survived'].count()).round(2)\nprint(\"Imbalnce ratio: {:}\".format(imbalanced_ratio))\n\nx_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=SEED)\nprint(\"Distribution of train and test:\", len(x_train), len(y_train), len(x_test), len(x_test))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Trial=0\ndef objective(trial, x=x_train, y=y_train):\n    global Trial\n    \n    para={\n        'verbosity': 1,\n        'objective': 'binary:logistic',\n        'random_state': SEED,\n        'seed': SEED,\n        'tree_method':'hist',\n        'scale_pos_weight': imbalanced_ratio,\n        'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n        'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.1, 1),\n        'subsample': trial.suggest_float('subsample', 0.1, 1),\n        'learning_rate': trial.suggest_float('learning_rate', 1e-4, 1e-1),\n        'n_estimators': trial.suggest_int('n_estimators',500, 20000),\n        'max_depth': trial.suggest_int('max_depth', 1, 31),\n        'min_child_weight': trial.suggest_int('min_child_weight', 1, 1000)\n    }\n    \n    print(\"--------------------> Trial {} <--------------------\".format(Trial))\n    Trial=Trial + 1\n    xgboost_train_preds = np.zeros(len(y),)\n    for fold, (train_ind, val_ind) in enumerate(kf.split(x, y)):\n        callbacks=[early_stopping_round]\n        print(\"--> Fold {}\".format(fold + 1))\n        xtrain, xval = x.iloc[train_ind], x.iloc[val_ind]\n        ytrain, yval = y.iloc[train_ind], y.iloc[val_ind]\n        xgboost = XGBClassifier(**para)\n        \n        model =  xgboost.fit(xtrain, ytrain, eval_set=[(xtrain,ytrain), (xval,yval)], \n                             eval_metric=[\"error\", \"logloss\"],verbose=0,callbacks=[early_stopping_round])\n        pred_train = model.predict(xtrain)\n        pred_val = model.predict(xval)\n        xgboost_train_preds[val_ind]=pred_val\n        score1 = accuracy_score(ytrain, pred_train)\n        score2 = accuracy_score(yval, pred_val)\n        print('Fold {} ACCURACY Train: {} Validation: {}'.format(fold+1, score1, score2))\n        \n        results = model.evals_result()\n        df=pd.DataFrame({\n                        \"validation_train_ll\":results[\"validation_0\"][\"logloss\"],\n                        \"validation_test_ll\":results[\"validation_1\"][\"logloss\"],\n                        \"validation_train_acc\":results[\"validation_0\"][\"error\"],\n                        \"validation_test_acc\":results[\"validation_1\"][\"error\"],\n                        \n        })\n        df['validation_train_acc']=(1-df['validation_train_acc'])*100.0\n        df['validation_test_acc']=(1-df['validation_test_acc'])*100.0\n#         print(df.head())\n        \n        fig, ax = plt.subplots(ncols=2, figsize=(12,6))\n        sns.lineplot(data=df, x=df.index, y=\"validation_train_ll\", ax=ax[0], label=\"Train loss\")\n        sns.lineplot(data=df, x=df.index, y=\"validation_test_ll\", ax=ax[0], label=\"Test loss\")\n        sns.lineplot(data=df, x=df.index, y=\"validation_train_acc\", ax=ax[1], label=\"Train acc.\")\n        sns.lineplot(data=df, x=df.index, y=\"validation_test_acc\", ax=ax[1], label=\"Test acc.\")\n        ax[0].set_title(\"Loss curve\")\n        ax[1].set_title(\"Accuracy curve\")\n        ax[0].set_ylabel(\"Loss\")\n        ax[0].set_xlabel(\"Itertation\")\n        ax[1].set_ylabel(\"Accuracy\")\n        ax[1].set_xlabel(\"Itertation\")\n        fig.suptitle(\"XGBoost Loss/Accuracy.\")\n        plt.show()\n    acc=accuracy_score(y, xgboost_train_preds)\n    print('OOF ACCURACY: {}'.format(acc))\n    return acc","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"study=optuna.create_study(study_name=\"XGBoost set 2 Optimization\", direction='maximize')\nstudy.optimize(objective, n_trials=50)\n\ntrial = study.best_trial\nprint('Accuracy: {}'.format(trial.value))\n\nprint(\"Best hyperparameters: {}\".format(trial.params))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h2 style=\"background-color:azure; text-align:center; font-size:200%\">XGBOOST set2 tuned model</h2>","metadata":{}},{"cell_type":"code","source":"para = {\n        'verbosity': 1,\n        'objective': 'binary:logistic',\n        'random_state': SEED,\n        'seed': SEED,\n        'tree_method':'hist',\n        'scale_pos_weight': imbalanced_ratio,\n        'lambda': 0.23870865587316725, \n        'alpha': 0.05851635206035666, \n        'colsample_bytree': 0.1050482977664344, \n        'subsample': 0.9719852687976757, \n        'learning_rate': 0.06744568400126143, \n        'n_estimators': 19537, 'max_depth': 3, \n        'min_child_weight': 498\n       }","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"xg_train_preds = np.zeros(len(y_train),)\nxg_test_preds = np.zeros(len(y_test),)\nxg_test = np.zeros(len(test2),)\nfor fold, (train_ind, val_ind) in enumerate(kf.split(x_train, y_train)):\n    print(\"--> Fold {}\".format(fold + 1))\n    \n    xtrain, xval = x_train.iloc[train_ind], x_train.iloc[val_ind]\n    ytrain, yval = y_train.iloc[train_ind], y_train.iloc[val_ind]\n    \n    xgboost = XGBClassifier(**para)\n\n    model = xgboost.fit(xtrain, ytrain, eval_set=[(xtrain,ytrain), (xval,yval)], \n                         eval_metric=[\"error\", \"logloss\"], verbose=0, early_stopping_rounds=50)\n    pred_train = model.predict_proba(xtrain)[:,1]\n    pred_val = model.predict_proba(xval)[:,1]\n    xg_train_preds[val_ind] = pred_val\n    xg_test_preds += (model.predict_proba(x_test)[:,1])/folds\n    xg_test += (model.predict_proba(test2)[:,1])/folds\n    score1 = accuracy_score(ytrain, np.where(pred_train<=0.5, 0, 1))\n    score2 = accuracy_score(yval, np.where(pred_val<=0.5, 0, 1))\n    print('Fold {} ACCURACY Train: {} Validation: {}'.format(fold+1, score1, score2))\n\n    results = model.evals_result()\n    df=pd.DataFrame({\n                    \"validation_train_ll\":results[\"validation_0\"][\"logloss\"],\n                    \"validation_test_ll\":results[\"validation_1\"][\"logloss\"],\n                    \"validation_train_acc\":results[\"validation_0\"][\"error\"],\n                    \"validation_test_acc\":results[\"validation_1\"][\"error\"],\n\n    })\n    df['validation_train_acc']=(1-df['validation_train_acc'])*100.0\n    df['validation_test_acc']=(1-df['validation_test_acc'])*100.0\n    #         print(df.head())\n\n    fig, ax = plt.subplots(ncols=2, figsize=(12,6))\n    sns.lineplot(data=df, x=df.index, y=\"validation_train_ll\", ax=ax[0], label=\"Train loss\")\n    sns.lineplot(data=df, x=df.index, y=\"validation_test_ll\", ax=ax[0], label=\"Test loss\")\n    sns.lineplot(data=df, x=df.index, y=\"validation_train_acc\", ax=ax[1], label=\"Train acc.\")\n    sns.lineplot(data=df, x=df.index, y=\"validation_test_acc\", ax=ax[1], label=\"Test acc.\")\n    ax[0].set_title(\"Loss curve\")\n    ax[1].set_title(\"Accuracy curve\")\n    ax[0].set_ylabel(\"Loss\")\n    ax[0].set_xlabel(\"Itertation\")\n    ax[1].set_ylabel(\"Accuracy\")\n    ax[1].set_xlabel(\"Itertation\")\n    fig.suptitle(\"XGBoost Loss/Accuracy.\")\n    plt.show()\n    \nacc1 = accuracy_score(y_train, np.where(xg_train_preds<=0.5, 0, 1))\nacc2 = accuracy_score(y_test, np.where(xg_test_preds<=0.5, 0, 1))\nprint('OOF ACCURACY Train: {} Test: {}'.format(acc1, acc2))\n\n# score_train['xg']=np.concatenate([xg_train_preds,xg_test_preds])\n# score_test['xg']=xg_test","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_=pd.read_csv('../input/tabular-playground-series-apr-2021/test.csv')\ndf=pd.DataFrame()\ndf['PassengerId']=test_['PassengerId'].values\ndf['Survived']=xg_test\ndf['Survived']=df['Survived'].apply(lambda x:0 if x<=0.5 else 1)\ndf.to_csv('./xg_tuned_set2.csv',index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<span style=\"background-color:orange; font-size:150%\">**Observation**</span>\n* For XGBOOST train set 1 gave better results in public leaderboard compared to train set 2.  ","metadata":{}},{"cell_type":"markdown","source":"<h2 style=\"background-color:azure; text-align:center; font-size:200%\">LightGBM</h2>","metadata":{}},{"cell_type":"code","source":"train1=pd.read_csv('../input/training-apr/train.csv')\ntest1=pd.read_csv('../input/training-apr/test.csv')\n\ntrain2=train1.copy()\ntest2=test1.copy()\n\ntrain1.drop(columns=['PassengerId','Name','Cabin','Ticket','SibSp','Parch','Age_log','Fare','Fare_cat','SibSp_log','Parch_log','related','related_cat'], inplace=True)\ntest1.drop(columns=['PassengerId','Name','Cabin','Ticket','SibSp','Parch','Age_log','Fare','Fare_cat','SibSp_log','Parch_log','related','related_cat'], inplace=True)\n\ntrain2.drop(columns=['PassengerId','Name','Cabin','Ticket','SibSp','Parch','Age_log','Fare','Fare_cat','SibSp_log','Parch_log','related','related_log'], inplace=True)\ntest2.drop(columns=['PassengerId','Name','Cabin','Ticket','SibSp','Parch','Age_log','Fare','Fare_cat','SibSp_log','Parch_log','related','related_log'], inplace=True)\n\nprint(\"set 1:\", train1.columns, test1.columns)\nprint(\"set 2:\", train2.columns, test2.columns)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<span style=\"background-color:orange; font-size:150%\">**Important Point**</span>\n* The cat features of lightgbm should only be used when you have high cardinality in categorical features. \n* It is common to represent categorical features with one-hot encoding, but this approach is suboptimal for tree learners. Particularly for high-cardinality categorical features, a tree built on one-hot features tends to be unbalanced and needs to grow very deep to achieve good accuracy.Instead of one-hot encoding, the optimal solution is to split on a categorical feature by partitioning its categories into 2 subsets. If the feature has k categories, there are 2^(k-1) - 1 possible partitions. But there is an efficient solution for regression trees. It needs about O(k * log(k)) to find the optimal partition.","metadata":{}},{"cell_type":"markdown","source":"<h2 style=\"background-color:azure; text-align:center; font-size:200%\">LightGBM set1</h2>","metadata":{}},{"cell_type":"code","source":"ohe=OneHotEncoder()\ncol=['Sex','Embarked']\nohe.fit(train1[col])\nprint(ohe.get_feature_names(col))\ndf1=pd.DataFrame(columns=ohe.get_feature_names(col),data=ohe.transform(train1[col]).toarray())\ndf2=pd.DataFrame(columns=ohe.get_feature_names(col),data=ohe.transform(test1[col]).toarray())\n\ntrain1=train1.join(df1)\ntest1=test1.join(df2)\n\ntrain1.drop(columns=['Sex','Embarked'], inplace=True)\ntest1.drop(columns=['Sex','Embarked'], inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train1.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test1.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"folds=5\nSEED=random.randint(937,8641)\n\nkf=StratifiedKFold(n_splits=folds, random_state=SEED, shuffle=True)\n\nfeatures=train1.columns[1:]\nX = train1[features]\ny = train1['Survived']\n\n\nimbalanced_ratio=(train1[train1['Survived']==0]['Survived'].count()/train1[train1['Survived']==1]['Survived'].count()).round(2)\nprint(\"Imbalnce ratio: {:}\".format(imbalanced_ratio))\nx_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=SEED)\n\nprint(\"Distribution of train and test:\", len(x_train), len(y_train), len(x_test), len(x_test))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Trial=0\n\ndef objective(trial, x=x_train, y=y_train):\n    global Trial\n    \n    para = {\n              'verbosity': 1,\n              'random_state': SEED,\n              'n_jobs': -1,\n              'is_unbalance': True,\n              'bagging_seed': SEED,\n              'feature_fraction_seed': SEED,\n              'objective': 'binary', \n              'boosting': trial.suggest_categorical('boosting', ['gbdt','rf']),\n              'n_estimators': trial.suggest_int('n_estimators',500, 20000),\n              'learning_rate': trial.suggest_float('learning_rate', 1e-4, 1e-1),\n              'max_depth': trial.suggest_int('max_depth', 6, 127),\n              'num_leaves': trial.suggest_int('num_leaves', 31, 128),\n              'reg_alpha': trial.suggest_float('reg_alpha', 1e-3, 10.0),\n              'reg_lambda': trial.suggest_float('reg_lambda', 1e-3, 10.0),\n              'feature_fraction': trial.suggest_float('feature_fraction', 0.2, 0.9),\n              'min_child_samples': trial.suggest_int('min_child_samples', 1, 300),\n              'bagging_freq': trial.suggest_int('bagging_freq', 50, 15000),\n              'bagging_fraction': trial.suggest_float('bagging_fraction', 0, 0.9),\n              'max_bin': trial.suggest_int('max_bin', 128, 1024)\n            }\n    \n    print(\"--------------------> Trial {} <--------------------\".format(Trial))\n    Trial=Trial + 1\n    lgbm_train_preds = np.zeros(len(y),)\n    \n    for fold, (train_ind, val_ind) in enumerate(kf.split(x, y)):\n        print(\"--> Fold {}\".format(fold + 1))\n        xtrain, xval = x.iloc[train_ind], x.iloc[val_ind]\n        ytrain, yval = y.iloc[train_ind], y.iloc[val_ind]\n        \n        lgbm = LGBMClassifier(**para)\n        \n        early_stopping = lightgbm.early_stopping(25, first_metric_only=True, verbose=True)\n        \n        model =  lgbm.fit(xtrain, ytrain, eval_set=[(xtrain,ytrain), (xval,yval)], categorical_feature=None,\n                          eval_metric = ['binary_logloss', 'binary_error'], verbose=100, callbacks=[early_stopping])\n        \n        pred_train = model.predict(xtrain, num_iteration=model.best_iteration_)\n        pred_val = model.predict(xval, num_iteration=model.best_iteration_)\n        lgbm_train_preds[val_ind]=pred_val\n        score1 = accuracy_score(ytrain, pred_train)\n        score2 = accuracy_score(yval, pred_val)\n        print('Fold {} ACCURACY Train: {} Validation: {}'.format(fold+1, score1, score2))\n        \n        results = model.evals_result_\n        df=pd.DataFrame({\n                        \"train_ll\":results[\"training\"][\"binary_logloss\"],\n                        \"validation_ll\":results[\"valid_1\"][\"binary_logloss\"],\n                        \"train_acc\":results[\"training\"][\"binary_error\"],\n                        \"test_acc\":results[\"valid_1\"][\"binary_error\"],\n        })\n        df['train_acc']=(1-df['train_acc'])*100.0\n        df['test_acc']=(1-df['test_acc'])*100.0\n#         print(df.head())\n        \n        fig, ax = plt.subplots(ncols=2, figsize=(12,6))\n        sns.lineplot(data=df, x=df.index, y=\"train_ll\", ax=ax[0], label=\"Train loss\")\n        sns.lineplot(data=df, x=df.index, y=\"validation_ll\", ax=ax[0], label=\"Validation loss\")\n        sns.lineplot(data=df, x=df.index, y=\"train_acc\", ax=ax[1], label=\"Train acc.\")\n        sns.lineplot(data=df, x=df.index, y=\"test_acc\", ax=ax[1], label=\"Validation acc.\")\n        ax[0].set_title(\"Loss curve\")\n        ax[1].set_title(\"Accuracy curve\")\n        ax[0].set_ylabel(\"Loss\")\n        ax[0].set_xlabel(\"Itertation\")\n        ax[1].set_ylabel(\"Accuracy\")\n        ax[1].set_xlabel(\"Itertation\")\n        fig.suptitle(\"LightGBM Loss/Accuracy.\")\n        plt.show()\n    acc=accuracy_score(y, lgbm_train_preds)\n    print('OOF ACCURACY: {}'.format(acc))\n    return acc","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"study=optuna.create_study(study_name=\"LightGBM set 1 Optimization\", direction='maximize')\nstudy.optimize(objective, n_trials=25)\n\ntrial = study.best_trial\nprint('Accuracy: {}'.format(trial.value))\n\nprint(\"Best hyperparameters: {}\".format(trial.params))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h2 style=\"background-color:azure; text-align:center; font-size:200%\">LightGBM set1 tuned model</h2>","metadata":{}},{"cell_type":"code","source":"para = {\n        'verbosity': 1,\n        'random_state': SEED,\n        'n_jobs': -1,\n        'is_unbalance': True,\n        'bagging_seed': SEED,\n        'feature_fraction_seed': SEED,\n        'objective': 'binary',\n        'boosting': 'gbdt', \n        'n_estimators': 12792, \n        'learning_rate': 0.0998109583959103, \n        'max_depth': 67, \n        'num_leaves': 81, \n        'reg_alpha': 8.95628715211493, \n        'reg_lambda': 2.5201711907717668, \n        'feature_fraction': 0.591553893731912, \n        'min_child_samples': 206, \n        'bagging_freq': 2628, \n        'bagging_fraction': 0.661876388933661, \n        'max_bin': 230\n       }","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lg_train_preds = np.zeros(len(y_train),)\nlg_test_preds = np.zeros(len(y_test),)\nlg_test = np.zeros(len(test1),)\nfor fold, (train_ind, val_ind) in enumerate(kf.split(x_train, y_train)):\n    print(\"--> Fold {}\".format(fold + 1))\n    \n    xtrain, xval = x_train.iloc[train_ind], x_train.iloc[val_ind]\n    ytrain, yval = y_train.iloc[train_ind], y_train.iloc[val_ind]\n    \n    early_stopping_round = lightgbm.early_stopping(25, first_metric_only=True, verbose=True)\n    \n    lgbm = LGBMClassifier(**para)\n        \n    model =  lgbm.fit(xtrain, ytrain, eval_set=[(xtrain,ytrain), (xval,yval)], categorical_feature=None,\n                      eval_metric = ['binary_logloss', 'binary_error'], verbose=100, callbacks=[early_stopping_round])\n    pred_train = model.predict_proba(xtrain, num_iteration=model.best_iteration_)[:,1]\n    pred_val = model.predict_proba(xval, num_iteration=model.best_iteration_)[:,1]\n    lg_train_preds[val_ind] = pred_val\n    lg_test_preds += (model.predict_proba(x_test, num_iteration=model.best_iteration_)[:,1])/folds\n    lg_test += (model.predict_proba(test1, num_iteration=model.best_iteration_)[:,1])/folds\n    score1 = accuracy_score(ytrain, np.where(pred_train<=0.5, 0, 1))\n    score2 = accuracy_score(yval, np.where(pred_val<=0.5, 0, 1))\n    print('Fold {} ACCURACY Train: {} Validation: {}'.format(fold+1, score1, score2))\n\n    results = model.evals_result_\n    df=pd.DataFrame({\n                    \"train_ll\":results[\"training\"][\"binary_logloss\"],\n                    \"validation_ll\":results[\"valid_1\"][\"binary_logloss\"],\n                    \"train_acc\":results[\"training\"][\"binary_error\"],\n                    \"test_acc\":results[\"valid_1\"][\"binary_error\"],\n\n    })\n    df['train_acc']=(1-df['train_acc'])*100.0\n    df['test_acc']=(1-df['test_acc'])*100.0\n#         print(df.head())\n\n    fig, ax = plt.subplots(ncols=2, figsize=(12,6))\n    sns.lineplot(data=df, x=df.index, y=\"train_ll\", ax=ax[0], label=\"Train loss\")\n    sns.lineplot(data=df, x=df.index, y=\"validation_ll\", ax=ax[0], label=\"Validation loss\")\n    sns.lineplot(data=df, x=df.index, y=\"train_acc\", ax=ax[1], label=\"Train acc.\")\n    sns.lineplot(data=df, x=df.index, y=\"test_acc\", ax=ax[1], label=\"Validation acc.\")\n    ax[0].set_title(\"Loss curve\")\n    ax[1].set_title(\"Accuracy curve\")\n    ax[0].set_ylabel(\"Loss\")\n    ax[0].set_xlabel(\"Itertation\")\n    ax[1].set_ylabel(\"Accuracy\")\n    ax[1].set_xlabel(\"Itertation\")\n    fig.suptitle(\"LightGBM Loss/Accuracy.\")\n    plt.show()\n    \nacc1 = accuracy_score(y_train, np.where(lg_train_preds<=0.5, 0, 1))\nacc2 = accuracy_score(y_test, np.where(lg_test_preds<=0.5, 0, 1))\n\nprint('OOF ACCURACY Train: {} Test: {}'.format(acc1, acc2))\n\nscore_train=pd.read_csv('../input/score-apr21/score_train.csv')\nscore_test=pd.read_csv('../input/score-apr21/score_test.csv')\n\nscore_train['lg']=np.concatenate([lg_train_preds,lg_test_preds])\nscore_test['lg']=lg_test","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"score_train.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"score_test.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"score_train.to_csv('./score_train.csv',index=False)\nscore_test.to_csv('./score_test.csv',index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_=pd.read_csv('../input/tabular-playground-series-apr-2021/test.csv')\ndf=pd.DataFrame()\ndf['PassengerId']=test_['PassengerId'].values\ndf['Survived']=lg_test\ndf['Survived']=df['Survived'].apply(lambda x:0 if x<=0.5 else 1)\ndf.to_csv('./lg_tuned.csv',index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h2 style=\"background-color:azure; text-align:center; font-size:200%\">LightGBM set2</h2>","metadata":{}},{"cell_type":"code","source":"ohe=OneHotEncoder()\ncol=['Sex','Embarked']\nohe.fit(train2[col])\nprint(ohe.get_feature_names(col))\ndf1=pd.DataFrame(columns=ohe.get_feature_names(col),data=ohe.transform(train2[col]).toarray())\ndf2=pd.DataFrame(columns=ohe.get_feature_names(col),data=ohe.transform(test2[col]).toarray())\n\ntrain2=train2.join(df1)\ntest2=test2.join(df2)\n\ntrain2['related_cat'] = train2['related_cat'].apply(lambda x:1 if x in ['low'] else 2)\ntest2['related_cat'] = test2['related_cat'].apply(lambda x:1 if x in ['low'] else 2)\n\ntrain2.drop(columns=['Sex','Embarked'], inplace=True)\ntest2.drop(columns=['Sex','Embarked'], inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train2.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test2.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"folds=5\nSEED=random.randint(937,8641)\n\nkf=StratifiedKFold(n_splits=folds, random_state=SEED, shuffle=True)\n\nfeatures=train2.columns[1:]\nX = train2[features]\ny = train2['Survived']\n\n\nimbalanced_ratio=(train2[train2['Survived']==0]['Survived'].count()/train2[train2['Survived']==1]['Survived'].count()).round(2)\nprint(\"Imbalnce ratio: {:}\".format(imbalanced_ratio))\n\nx_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=SEED)\nprint(\"Distribution of train and test:\", len(x_train), len(y_train), len(x_test), len(x_test))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Trial=0\n\n\ndef objective(trial, x=x_train, y=y_train):\n    global Trial\n    \n    para = {\n              'verbosity': 1,\n              'random_state': SEED,\n              'n_jobs': -1,\n              'is_unbalance': True,\n              'bagging_seed': SEED,\n              'feature_fraction_seed': SEED,\n              'objective': 'binary',\n              'boosting': trial.suggest_categorical('boosting', ['gbdt','rf']),\n              'n_estimators': trial.suggest_int('n_estimators',500, 20000),\n              'learning_rate': trial.suggest_float('learning_rate', 1e-4, 1e-1),\n              'max_depth': trial.suggest_int('max_depth', 6, 127),\n              'num_leaves': trial.suggest_int('num_leaves', 31, 128),\n              'reg_alpha': trial.suggest_float('reg_alpha', 1e-3, 10.0),\n              'reg_lambda': trial.suggest_float('reg_lambda', 1e-3, 10.0),\n              'feature_fraction': trial.suggest_float('feature_fraction', 0.2, 0.9),\n              'min_child_samples': trial.suggest_int('min_child_samples', 1, 300),\n              'bagging_freq': trial.suggest_int('bagging_freq', 50, 15000),\n              'bagging_fraction': trial.suggest_float('bagging_fraction', 0, 0.9),\n              'max_bin': trial.suggest_int('max_bin', 128, 1024)\n            }\n    \n    print(\"--------------------> Trial {} <--------------------\".format(Trial))\n    Trial=Trial + 1\n    lgbm_train_preds = np.zeros(len(y),)\n    for fold, (train_ind, val_ind) in enumerate(kf.split(x, y)):\n        print(\"--> Fold {}\".format(fold + 1))\n        xtrain, xval = x.iloc[train_ind], x.iloc[val_ind]\n        ytrain, yval = y.iloc[train_ind], y.iloc[val_ind]\n        lgbm = LGBMClassifier(**para)\n        \n        early_stopping_round = lightgbm.early_stopping(25, first_metric_only=True, verbose=True)\n        model =  lgbm.fit(xtrain, ytrain, eval_set=[(xtrain,ytrain), (xval,yval)], categorical_feature=None,\n                             eval_metric = ['binary_logloss', 'binary_error'], verbose=100, callbacks=[early_stopping_round])\n        \n        pred_train = model.predict(xtrain, num_iteration=model.best_iteration_)\n        pred_val = model.predict(xval, num_iteration=model.best_iteration_)\n        lgbm_train_preds[val_ind]=pred_val\n        score1 = accuracy_score(ytrain, pred_train)\n        score2 = accuracy_score(yval, pred_val)\n        print('Fold {} ACCURACY Train: {} Validation: {}'.format(fold+1, score1, score2))\n        \n        results = model.evals_result_\n        df=pd.DataFrame({\n                        \"train_ll\":results[\"training\"][\"binary_logloss\"],\n                        \"validation_ll\":results[\"valid_1\"][\"binary_logloss\"],\n                        \"train_acc\":results[\"training\"][\"binary_error\"],\n                        \"test_acc\":results[\"valid_1\"][\"binary_error\"],\n                        \n        })\n        df['train_acc']=(1-df['train_acc'])*100.0\n        df['test_acc']=(1-df['test_acc'])*100.0\n#         print(df.head())\n        \n        fig, ax = plt.subplots(ncols=2, figsize=(12,6))\n        sns.lineplot(data=df, x=df.index, y=\"train_ll\", ax=ax[0], label=\"Train loss\")\n        sns.lineplot(data=df, x=df.index, y=\"validation_ll\", ax=ax[0], label=\"Validation loss\")\n        sns.lineplot(data=df, x=df.index, y=\"train_acc\", ax=ax[1], label=\"Train acc.\")\n        sns.lineplot(data=df, x=df.index, y=\"test_acc\", ax=ax[1], label=\"Validation acc.\")\n        ax[0].set_title(\"Loss curve\")\n        ax[1].set_title(\"Accuracy curve\")\n        ax[0].set_ylabel(\"Loss\")\n        ax[0].set_xlabel(\"Itertation\")\n        ax[1].set_ylabel(\"Accuracy\")\n        ax[1].set_xlabel(\"Itertation\")\n        fig.suptitle(\"LightGBM Loss/Accuracy.\")\n        plt.show()\n    acc=accuracy_score(y, lgbm_train_preds)\n    print('OOF ACCURACY: {}'.format(acc))\n    return acc","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"study=optuna.create_study(study_name=\"LightGBM set 2 Optimization\", direction='maximize')\nstudy.optimize(objective, n_trials=25)\n\ntrial = study.best_trial\nprint('Accuracy: {}'.format(trial.value))\n\nprint(\"Best hyperparameters: {}\".format(trial.params))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h2 style=\"background-color:azure; text-align:center; font-size:200%\">LightGBM set2 tuned model</h2>","metadata":{}},{"cell_type":"code","source":"para = {\n        'verbosity': 1,\n        'random_state': SEED,\n        'n_jobs': -1,\n        'is_unbalance': True,\n        'bagging_seed': SEED,\n        'feature_fraction_seed': SEED,\n        'objective': 'binary',\n        'boosting': 'gbdt', \n        'n_estimators': 10701, \n        'learning_rate': 0.06955447670117614, \n        'max_depth': 65, 'num_leaves': 110, \n        'reg_alpha': 8.557750007560998, \n        'reg_lambda': 0.016304042294640997, \n        'feature_fraction': 0.2779304475599476, \n        'min_child_samples': 233, \n        'bagging_freq': 14930, \n        'bagging_fraction': 0.8838910149186466, \n        'max_bin': 370\n       }","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lg_train_preds = np.zeros(len(y_train),)\nlg_test_preds = np.zeros(len(y_test),)\nlg_test = np.zeros(len(test2),)\nfor fold, (train_ind, val_ind) in enumerate(kf.split(x_train, y_train)):\n    print(\"--> Fold {}\".format(fold + 1))\n    \n    xtrain, xval = x_train.iloc[train_ind], x_train.iloc[val_ind]\n    ytrain, yval = y_train.iloc[train_ind], y_train.iloc[val_ind]\n    \n    early_stopping_round = lightgbm.early_stopping(25, first_metric_only=True, verbose=True)\n    \n    lgbm = LGBMClassifier(**para)\n        \n    model =  lgbm.fit(xtrain, ytrain, eval_set=[(xtrain,ytrain), (xval,yval)], categorical_feature=None,\n                      eval_metric = ['binary_logloss', 'binary_error'], verbose=100, callbacks=[early_stopping_round])\n    pred_train = model.predict_proba(xtrain, num_iteration=model.best_iteration_)[:,1]\n    pred_val = model.predict_proba(xval, num_iteration=model.best_iteration_)[:,1]\n    lg_train_preds[val_ind] = pred_val\n    lg_test_preds += (model.predict_proba(x_test, num_iteration=model.best_iteration_)[:,1])/folds\n    lg_test += (model.predict_proba(test2, num_iteration=model.best_iteration_)[:,1])/folds\n    score1 = accuracy_score(ytrain, np.where(pred_train<=0.5, 0, 1))\n    score2 = accuracy_score(yval, np.where(pred_val<=0.5, 0, 1))\n    print('Fold {} ACCURACY Train: {} Validation: {}'.format(fold+1, score1, score2))\n\n    results = model.evals_result_\n    df=pd.DataFrame({\n                    \"train_ll\":results[\"training\"][\"binary_logloss\"],\n                    \"validation_ll\":results[\"valid_1\"][\"binary_logloss\"],\n                    \"train_acc\":results[\"training\"][\"binary_error\"],\n                    \"test_acc\":results[\"valid_1\"][\"binary_error\"],\n\n    })\n    df['train_acc']=(1-df['train_acc'])*100.0\n    df['test_acc']=(1-df['test_acc'])*100.0\n#         print(df.head())\n\n    fig, ax = plt.subplots(ncols=2, figsize=(12,6))\n    sns.lineplot(data=df, x=df.index, y=\"train_ll\", ax=ax[0], label=\"Train loss\")\n    sns.lineplot(data=df, x=df.index, y=\"validation_ll\", ax=ax[0], label=\"Validation loss\")\n    sns.lineplot(data=df, x=df.index, y=\"train_acc\", ax=ax[1], label=\"Train acc.\")\n    sns.lineplot(data=df, x=df.index, y=\"test_acc\", ax=ax[1], label=\"Validation acc.\")\n    ax[0].set_title(\"Loss curve\")\n    ax[1].set_title(\"Accuracy curve\")\n    ax[0].set_ylabel(\"Loss\")\n    ax[0].set_xlabel(\"Itertation\")\n    ax[1].set_ylabel(\"Accuracy\")\n    ax[1].set_xlabel(\"Itertation\")\n    fig.suptitle(\"LightGBM Loss/Accuracy.\")\n    plt.show()\n    \nacc1 = accuracy_score(y_train, np.where(lg_train_preds<=0.5, 0, 1))\nacc2 = accuracy_score(y_test, np.where(lg_test_preds<=0.5, 0, 1))\n\nprint('OOF ACCURACY Train: {} Test: {}'.format(acc1, acc2))\n\n# score_train=pd.read_csv('../input/score-apr21/score_train.csv')\n# score_test=pd.read_csv('../input/score-apr21/score_test.csv')\n\n# score_train['lg']=np.concatenate([lg_train_preds,lg_test_preds])\n# score_test['lg']=lg_test","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_=pd.read_csv('../input/tabular-playground-series-apr-2021/test.csv')\ndf=pd.DataFrame()\ndf['PassengerId']=test_['PassengerId'].values\ndf['Survived']=lg_test\ndf['Survived']=df['Survived'].apply(lambda x:0 if x<=0.5 else 1)\ndf.to_csv('./lg_tuned_set2.csv',index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<span style=\"background-color:orange; font-size:150%\">**Observation**</span>\n* For LightGBM train set 1 gave better results in public leaderboard compared to train set 2.","metadata":{}},{"cell_type":"markdown","source":"<h2 style=\"background-color:azure; text-align:center; font-size:200%\">Random Forest</h2>","metadata":{}},{"cell_type":"code","source":"train1=pd.read_csv('../input/training-apr/train.csv')\ntest1=pd.read_csv('../input/training-apr/test.csv')\n\ntrain2=train1.copy()\ntest2=test1.copy()\n\ntrain1.drop(columns=['PassengerId','Name','Cabin','Ticket','SibSp','Parch','Age_log','Fare','Fare_cat','SibSp_log','Parch_log','related','related_cat'], inplace=True)\ntest1.drop(columns=['PassengerId','Name','Cabin','Ticket','SibSp','Parch','Age_log','Fare','Fare_cat','SibSp_log','Parch_log','related','related_cat'], inplace=True)\n\ntrain2.drop(columns=['PassengerId','Name','Cabin','Ticket','SibSp','Parch','Age_log','Fare','Fare_cat','SibSp_log','Parch_log','related','related_log'], inplace=True)\ntest2.drop(columns=['PassengerId','Name','Cabin','Ticket','SibSp','Parch','Age_log','Fare','Fare_cat','SibSp_log','Parch_log','related','related_log'], inplace=True)\n\nprint(\"set 1:\", train1.columns, test1.columns)\nprint(\"set 2:\", train2.columns, test2.columns)","metadata":{"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"set 1: Index(['Survived', 'Pclass', 'Sex', 'Age', 'Embarked', 'Fare_log',\n       'related_log'],\n      dtype='object') Index(['Pclass', 'Sex', 'Age', 'Embarked', 'Fare_log', 'related_log'], dtype='object')\nset 2: Index(['Survived', 'Pclass', 'Sex', 'Age', 'Embarked', 'Fare_log',\n       'related_cat'],\n      dtype='object') Index(['Pclass', 'Sex', 'Age', 'Embarked', 'Fare_log', 'related_cat'], dtype='object')\n","output_type":"stream"}]},{"cell_type":"markdown","source":"<h2 style=\"background-color:azure; text-align:center; font-size:200%\">Random Forest set1</h2>","metadata":{}},{"cell_type":"code","source":"ohe=OneHotEncoder()\ncol=['Sex','Embarked']\nohe.fit(train1[col])\nprint(ohe.get_feature_names(col))\ndf1=pd.DataFrame(columns=ohe.get_feature_names(col),data=ohe.transform(train1[col]).toarray())\ndf2=pd.DataFrame(columns=ohe.get_feature_names(col),data=ohe.transform(test1[col]).toarray())\n\ntrain1=train1.join(df1)\ntest1=test1.join(df2)\n\ntrain1.drop(columns=['Sex','Embarked'], inplace=True)\ntest1.drop(columns=['Sex','Embarked'], inplace=True)","metadata":{"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"['Sex_female' 'Sex_male' 'Embarked_C' 'Embarked_Q' 'Embarked_S'\n 'Embarked_X']\n","output_type":"stream"}]},{"cell_type":"code","source":"train1.head()","metadata":{"trusted":true},"execution_count":4,"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"   Survived  Pclass    Age  Fare_log  related_log  Sex_female  Sex_male  \\\n0         1       3  36.00  3.337192     1.098612         0.0       1.0   \n1         0       1  36.00  2.663750     0.000000         0.0       1.0   \n2         0       1   0.33  4.280686     1.386294         0.0       1.0   \n3         0       1  19.00  2.641910     0.000000         0.0       1.0   \n4         1       1  25.00  2.170196     0.000000         0.0       1.0   \n\n   Embarked_C  Embarked_Q  Embarked_S  Embarked_X  \n0         0.0         0.0         1.0         0.0  \n1         0.0         0.0         1.0         0.0  \n2         0.0         0.0         1.0         0.0  \n3         0.0         0.0         1.0         0.0  \n4         0.0         0.0         1.0         0.0  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Survived</th>\n      <th>Pclass</th>\n      <th>Age</th>\n      <th>Fare_log</th>\n      <th>related_log</th>\n      <th>Sex_female</th>\n      <th>Sex_male</th>\n      <th>Embarked_C</th>\n      <th>Embarked_Q</th>\n      <th>Embarked_S</th>\n      <th>Embarked_X</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>3</td>\n      <td>36.00</td>\n      <td>3.337192</td>\n      <td>1.098612</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n      <td>1</td>\n      <td>36.00</td>\n      <td>2.663750</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0</td>\n      <td>1</td>\n      <td>0.33</td>\n      <td>4.280686</td>\n      <td>1.386294</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0</td>\n      <td>1</td>\n      <td>19.00</td>\n      <td>2.641910</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1</td>\n      <td>1</td>\n      <td>25.00</td>\n      <td>2.170196</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"test1.head()","metadata":{"trusted":true},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"   Pclass   Age  Fare_log  related_log  Sex_female  Sex_male  Embarked_C  \\\n0       1  19.0  4.159039     0.000000         0.0       1.0         0.0   \n1       1  53.0  1.918392     0.000000         1.0       0.0         0.0   \n2       3  19.0  3.686627     0.000000         1.0       0.0         1.0   \n3       2  25.0  2.634045     0.000000         0.0       1.0         0.0   \n4       3  17.0  3.328268     1.098612         1.0       0.0         1.0   \n\n   Embarked_Q  Embarked_S  Embarked_X  \n0         0.0         1.0         0.0  \n1         0.0         1.0         0.0  \n2         0.0         0.0         0.0  \n3         0.0         1.0         0.0  \n4         0.0         0.0         0.0  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Pclass</th>\n      <th>Age</th>\n      <th>Fare_log</th>\n      <th>related_log</th>\n      <th>Sex_female</th>\n      <th>Sex_male</th>\n      <th>Embarked_C</th>\n      <th>Embarked_Q</th>\n      <th>Embarked_S</th>\n      <th>Embarked_X</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>19.0</td>\n      <td>4.159039</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>53.0</td>\n      <td>1.918392</td>\n      <td>0.000000</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>19.0</td>\n      <td>3.686627</td>\n      <td>0.000000</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2</td>\n      <td>25.0</td>\n      <td>2.634045</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>3</td>\n      <td>17.0</td>\n      <td>3.328268</td>\n      <td>1.098612</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"folds=5\nSEED=random.randint(937,8641)\n\nkf=StratifiedKFold(n_splits=folds, random_state=SEED, shuffle=True)\n\nfeatures=train1.columns[1:]\nX = train1[features]\ny = train1['Survived']\n\n\nimbalanced_ratio=(train1[train1['Survived']==0]['Survived'].count()/train1[train1['Survived']==1]['Survived'].count()).round(2)\nprint(\"Imbalnce ratio: {:}\".format(imbalanced_ratio))\nx_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=SEED)\n\nprint(\"Distribution of train and test:\", len(x_train), len(y_train), len(x_test), len(x_test))","metadata":{"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"Imbalnce ratio: 1.34\nDistribution of train and test: 80000 80000 20000 20000\n","output_type":"stream"}]},{"cell_type":"code","source":"Trial=0\ndef objective(trial, x=x_train, y=y_train):\n    global Trial\n    \n    para={\n        'bootstrap': True,\n        'n_jobs': -1,\n        'verbose': 0,\n        'random_state': SEED,\n        'criterion': 'entropy',\n        'n_estimators': trial.suggest_int('n_estimators',10, 1000),\n        'max_depth': trial.suggest_int('max_depth', 3, 2000),\n        'min_samples_split': trial.suggest_float('min_samples_split', 1e-4, 1e-1),\n        'min_samples_leaf': trial.suggest_float('min_samples_leaf', 1e-4, 1e-1),\n        'max_features': trial.suggest_categorical(\"max_features\", ['sqrt', 'log2']),\n        'max_leaf_nodes': trial.suggest_int('max_leaf_nodes', 500, 100000),\n        'warm_start': trial.suggest_categorical('warm_start', [True, False]),\n        'min_impurity_decrease': trial.suggest_float('min_impurity_decrease', 0.0, 0.2),\n        'class_weight':trial.suggest_categorical(\"class_weight\", ['balanced', 'balanced_subsample']),\n        'max_samples': trial.suggest_float('max_samples', 0.5, 0.8)\n    }\n    \n    print(\"--------------------> Trial {} <--------------------\".format(Trial))\n    Trial=Trial + 1\n    rf_train_preds = np.zeros(len(y),)\n    for fold, (train_ind, val_ind) in enumerate(kf.split(x, y)):\n        print(\"--> Fold {}\".format(fold + 1))\n        xtrain, xval = x.iloc[train_ind], x.iloc[val_ind]\n        ytrain, yval = y.iloc[train_ind], y.iloc[val_ind]\n        \n        rf = RandomForestClassifier(**para)\n\n        model =  rf.fit(xtrain, ytrain)\n        \n        pred_train = model.predict(xtrain)\n        pred_val = model.predict(xval)\n        rf_train_preds[val_ind]=pred_val\n        score1 = accuracy_score(ytrain, pred_train)\n        score2 = accuracy_score(yval, pred_val)\n        print('Fold {} AUC Train: {} Validation: {}'.format(fold+1, score1, score2))\n    \n    acc=accuracy_score(y, rf_train_preds)\n    print('OOF ACCURACY: {}'.format(acc))\n    return acc","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"study=optuna.create_study(study_name=\"Random Forest set 1 Optimization\", direction='maximize')\nstudy.optimize(objective, n_trials=25)\n\ntrial = study.best_trial\nprint('Accuracy: {}'.format(trial.value))\n\nprint(\"Best hyperparameters: {}\".format(trial.params))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Trial=0\ndef objective(trial, x=x_train, y=y_train):\n    global Trial\n    \n    para={\n        'bootstrap': True,\n        'n_jobs': -1,\n        'verbose': 0,\n        'random_state': SEED,\n        'criterion': 'entropy',\n        'n_estimators': 330, \n        'max_depth': 1417, \n        'min_samples_split': 0.010377003772355852, \n        'min_samples_leaf': 0.01865186498148891, \n        'max_features': 'log2', \n        'max_leaf_nodes': 87127, \n        'warm_start': False, \n        'min_impurity_decrease': 0.01122129357981094, \n        'class_weight': 'balanced_subsample', \n        'max_samples': 0.5985586520207642,\n        'ccp_alpha': trial.suggest_float('ccp_alpha', 0.0, 2e-1)\n    }\n    \n    print(\"--------------------> Trial {} <--------------------\".format(Trial))\n    Trial=Trial + 1\n    rf_train_preds = np.zeros(len(y),)\n    for fold, (train_ind, val_ind) in enumerate(kf.split(x, y)):\n        print(\"--> Fold {}\".format(fold + 1))\n        xtrain, xval = x.iloc[train_ind], x.iloc[val_ind]\n        ytrain, yval = y.iloc[train_ind], y.iloc[val_ind]\n        \n        rf = RandomForestClassifier(**para)\n\n        model =  rf.fit(xtrain, ytrain)\n        \n        pred_train = model.predict(xtrain)\n        pred_val = model.predict(xval)\n        rf_train_preds[val_ind]=pred_val\n        score1 = accuracy_score(ytrain, pred_train)\n        score2 = accuracy_score(yval, pred_val)\n        print('Fold {} AUC Train: {} Validation: {}'.format(fold+1, score1, score2))\n    \n    acc=accuracy_score(y, rf_train_preds)\n    print('OOF ACCURACY: {}'.format(acc))\n    return acc","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"study=optuna.create_study(study_name=\"Random Forest set 1 Pruning\", direction='maximize')\nstudy.optimize(objective, n_trials=25)\n\ntrial = study.best_trial\nprint('Accuracy: {}'.format(trial.value))\n\nprint(\"Best hyperparameters: {}\".format(trial.params))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h2 style=\"background-color:azure; text-align:center; font-size:200%\">Random Forest set1 tuned model</h2>","metadata":{}},{"cell_type":"code","source":"para = {\n        'bootstrap': True,\n        'n_jobs': -1,\n        'verbose': 0,\n        'random_state': SEED,\n        'criterion': 'entropy',\n        'n_estimators': 330, \n        'max_depth': 1417, \n        'min_samples_split': 0.010377003772355852, \n        'min_samples_leaf': 0.01865186498148891, \n        'max_features': 'log2', \n        'max_leaf_nodes': 87127, \n        'warm_start': False, \n        'min_impurity_decrease': 0.01122129357981094, \n        'class_weight': 'balanced_subsample', \n        'max_samples': 0.5985586520207642,\n        'ccp_alpha': 0.018500631807288694\n       }","metadata":{"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"rf_train_preds = np.zeros(len(y_train),)\nrf_test_preds = np.zeros(len(y_test),)\nrf_test = np.zeros(len(test1),)\nfor fold, (train_ind, val_ind) in enumerate(kf.split(x_train, y_train)):\n    print(\"--> Fold {}\".format(fold + 1))\n    \n    xtrain, xval = x_train.iloc[train_ind], x_train.iloc[val_ind]\n    ytrain, yval = y_train.iloc[train_ind], y_train.iloc[val_ind]\n    \n    rf = RandomForestClassifier(**para)\n\n    model =  rf.fit(xtrain, ytrain)\n    \n    pred_train = model.predict_proba(xtrain)[:,1]\n    pred_val = model.predict_proba(xval)[:,1]\n    rf_train_preds[val_ind] = pred_val\n    rf_test_preds += (model.predict_proba(x_test)[:,1])/folds\n    rf_test += (model.predict_proba(test1)[:,1])/folds\n    score1 = accuracy_score(ytrain, np.where(pred_train<=0.5, 0, 1))\n    score2 = accuracy_score(yval, np.where(pred_val<=0.5, 0, 1))\n    print('Fold {} ACCURACY Train: {} Validation: {}'.format(fold+1, score1, score2))\n    \nacc1 = accuracy_score(y_train, np.where(rf_train_preds<=0.5, 0, 1))\nacc2 = accuracy_score(y_test, np.where(rf_test_preds<=0.5, 0, 1))\n\nprint('OOF ACCURACY Train: {} Test: {}'.format(acc1, acc2))\n\nscore_train=pd.read_csv('../input/score-tab-apr21/score_train.csv')\nscore_test=pd.read_csv('../input/score-tab-apr21/score_test.csv')\n\nscore_train['rf']=np.concatenate([rf_train_preds,rf_test_preds])\nscore_test['rf']=rf_test","metadata":{"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"--> Fold 1\nFold 1 ACCURACY Train: 0.764296875 Validation: 0.763375\n--> Fold 2\nFold 2 ACCURACY Train: 0.763375 Validation: 0.7670625\n--> Fold 3\nFold 3 ACCURACY Train: 0.76271875 Validation: 0.7696875\n--> Fold 4\nFold 4 ACCURACY Train: 0.764640625 Validation: 0.762\n--> Fold 5\nFold 5 ACCURACY Train: 0.76553125 Validation: 0.7584375\nOOF ACCURACY Train: 0.7641125 Test: 0.7712\n","output_type":"stream"}]},{"cell_type":"code","source":"score_train.head()","metadata":{"trusted":true},"execution_count":12,"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"         xg        rf        lg  cb\n0  0.893169  0.228584  0.223467 NaN\n1  0.760047  0.780058  0.307224 NaN\n2  0.787606  0.206423  0.673330 NaN\n3  0.555690  0.762223  0.142897 NaN\n4  0.294982  0.784771  0.365371 NaN","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>xg</th>\n      <th>rf</th>\n      <th>lg</th>\n      <th>cb</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.893169</td>\n      <td>0.228584</td>\n      <td>0.223467</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.760047</td>\n      <td>0.780058</td>\n      <td>0.307224</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.787606</td>\n      <td>0.206423</td>\n      <td>0.673330</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.555690</td>\n      <td>0.762223</td>\n      <td>0.142897</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.294982</td>\n      <td>0.784771</td>\n      <td>0.365371</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"score_test.head()","metadata":{"trusted":true},"execution_count":13,"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"         xg        rf        lg  cb\n0  0.165310  0.237214  0.127184 NaN\n1  0.607289  0.667611  0.601769 NaN\n2  0.935854  0.781036  0.930494 NaN\n3  0.256100  0.282167  0.228358 NaN\n4  0.813994  0.781036  0.801692 NaN","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>xg</th>\n      <th>rf</th>\n      <th>lg</th>\n      <th>cb</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.165310</td>\n      <td>0.237214</td>\n      <td>0.127184</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.607289</td>\n      <td>0.667611</td>\n      <td>0.601769</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.935854</td>\n      <td>0.781036</td>\n      <td>0.930494</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.256100</td>\n      <td>0.282167</td>\n      <td>0.228358</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.813994</td>\n      <td>0.781036</td>\n      <td>0.801692</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"score_train.to_csv('./score_train.csv',index=False)\nscore_test.to_csv('./score_test.csv',index=False)","metadata":{"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"test_=pd.read_csv('../input/tabular-playground-series-apr-2021/test.csv')\ndf=pd.DataFrame()\ndf['PassengerId']=test_['PassengerId'].values\ndf['Survived']=rf_test\ndf['Survived']=df['Survived'].apply(lambda x:0 if x<=0.5 else 1)\ndf.to_csv('./rf_tuned.csv',index=False)","metadata":{"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"markdown","source":"<h2 style=\"background-color:azure; text-align:center; font-size:200%\">Random Forest set2</h2>","metadata":{}},{"cell_type":"code","source":"ohe=OneHotEncoder()\ncol=['Sex','Embarked']\nohe.fit(train2[col])\nprint(ohe.get_feature_names(col))\ndf1=pd.DataFrame(columns=ohe.get_feature_names(col),data=ohe.transform(train2[col]).toarray())\ndf2=pd.DataFrame(columns=ohe.get_feature_names(col),data=ohe.transform(test2[col]).toarray())\n\ntrain2=train2.join(df1)\ntest2=test2.join(df2)\n\ntrain2['related_cat'] = train2['related_cat'].apply(lambda x:1 if x in ['low'] else 2)\ntest2['related_cat'] = test2['related_cat'].apply(lambda x:1 if x in ['low'] else 2)\n\ntrain2.drop(columns=['Sex','Embarked'], inplace=True)\ntest2.drop(columns=['Sex','Embarked'], inplace=True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train2.head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test2.head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"folds=5\nSEED=random.randint(937,8641)\n\nkf=StratifiedKFold(n_splits=folds, random_state=SEED, shuffle=True)\n\nfeatures=train2.columns[1:]\nX = train2[features]\ny = train2['Survived']\n\n\nimbalanced_ratio=(train2[train2['Survived']==0]['Survived'].count()/train2[train2['Survived']==1]['Survived'].count()).round(2)\nprint(\"Imbalnce ratio: {:}\".format(imbalanced_ratio))\n\nx_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=SEED)\nprint(\"Distribution of train and test:\", len(x_train), len(y_train), len(x_test), len(x_test))","metadata":{},"execution_count":null,"outputs":[]}]}